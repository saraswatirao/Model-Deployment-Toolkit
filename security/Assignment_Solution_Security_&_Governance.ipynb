{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Question:\n",
        "\n",
        "Utilize the tensorflow_privacy library to train a classification model on the MNIST dataset while employing the DPKerasSGDOptimizer as the training optimizer, ensuring that the training process is differentially private. Upon completion, calculate the epsilon value for the given training parameters.\n",
        "\n",
        "Please take note of the following instructions:\n",
        "\n",
        "*  Utilize the MNIST Dataset from Keras.datasets.\n",
        "*  Implement a CNN architecture.\n",
        "*  Suggested parameters for SGD Optimiser: batch_size = 250, l2_norm_clip = 1.5, noise_multiplier = 1.3, num_microbatches = 250, learning_rate = 0.25.\n",
        "*  You can utilize the compute_dp_sgd_privacy_statement function to determine the epsilon value.\n",
        "\n",
        "\n",
        "For reference, you can refer to the https://www.tensorflow.org/responsible_ai/privacy/guide."
      ],
      "metadata": {
        "id": "aF0iavUJgmvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-privacy"
      ],
      "metadata": {
        "id": "AA7EVcplgg_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7wQyhWMTgeOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = tf.keras.datasets.mnist.load_data()#load the dataset\n",
        "#train test split\n",
        "train_data, train_labels = train\n",
        "test_data, test_labels = test\n",
        "#normalise\n",
        "train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "#one hot encoding\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPpWlwvjihxl",
        "outputId": "d79725af-57f7-41f1-9507-6e08149befed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining parameters\n",
        "epochs = 5\n",
        "batch_size = 250\n",
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = 1.3\n",
        "num_microbatches = 250\n",
        "learning_rate = 0.25\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
      ],
      "metadata": {
        "id": "uzPVzKzgi-xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH1oSJNe0HcN"
      },
      "outputs": [],
      "source": [
        "#defining model using convolution and dense layer with softmax\n",
        "model = tf.keras.Sequential([\n",
        "    # Convolutional Layer 1\n",
        "    tf.keras.layers.Conv2D(16, 8,\n",
        "                           strides=2,\n",
        "                           padding='same',\n",
        "                           activation='relu',\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    # Max Pooling Layer 1\n",
        "    tf.keras.layers.MaxPool2D(2, 1),\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    tf.keras.layers.Conv2D(32, 4,\n",
        "                           strides=2,\n",
        "                           padding='valid',\n",
        "                           activation='relu'),\n",
        "    # Max Pooling Layer 2\n",
        "    tf.keras.layers.MaxPool2D(2, 1),\n",
        "\n",
        "    # Flatten Layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # Dense Layer 1\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    # Output Layer (Dense Layer 2)\n",
        "    # Note: The number of units in the output layer should match the number of classes in your classification task.\n",
        "    tf.keras.layers.Dense(10)  # Assuming 10 classes\n",
        "])\n",
        "#defining the optimiser with our parameters that we had defined earlier\n",
        "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "#defining the loss function as cateorical cross entropy\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "#training the model\n",
        "history=model.fit(train_data, train_labels,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_data, test_labels),\n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBQgRk6oj40t",
        "outputId": "244a1b29-3d7c-4b8b-cf2b-4d07460f2de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "240/240 [==============================] - 63s 256ms/step - loss: 0.5277 - accuracy: 0.8610 - val_loss: 0.4114 - val_accuracy: 0.8964\n",
            "Epoch 2/5\n",
            "240/240 [==============================] - 55s 228ms/step - loss: 0.4375 - accuracy: 0.8986 - val_loss: 0.3837 - val_accuracy: 0.9161\n",
            "Epoch 3/5\n",
            "240/240 [==============================] - 56s 231ms/step - loss: 0.3842 - accuracy: 0.9158 - val_loss: 0.3388 - val_accuracy: 0.9269\n",
            "Epoch 4/5\n",
            "240/240 [==============================] - 55s 229ms/step - loss: 0.3587 - accuracy: 0.9248 - val_loss: 0.3382 - val_accuracy: 0.9319\n",
            "Epoch 5/5\n",
            "240/240 [==============================] - 55s 229ms/step - loss: 0.3297 - accuracy: 0.9329 - val_loss: 0.2885 - val_accuracy: 0.9421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "#calculating epsilon (privacy budget) for our model parameters\n",
        "epsilon,_=tensorflow_privacy.compute_dp_sgd_privacy(n=train_data.shape[0],\n",
        "                                              batch_size=batch_size,\n",
        "                                              noise_multiplier=noise_multiplier,\n",
        "                                              epochs=epochs,\n",
        "                                              delta=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF8sh5YxkCke",
        "outputId": "4e923239-0275-4bca-ae89-0a69bfb67244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The epsilon for the differential training comes out to be 0.6353049965480387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also calculate\n",
        "tensorflow_privacy.compute_dp_sgd_privacy_statement(number_of_examples=train_data.shape[0],\n",
        "                                              batch_size=batch_size,\n",
        "                                              noise_multiplier=noise_multiplier,\n",
        "                                              used_microbatching=250,\n",
        "                                              num_epochs=epochs,\n",
        "                                              delta=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8SejR3qjnGGo",
        "outputId": "2c6caa36-8ab0-4b7f-a64f-b13879d5aee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DP-SGD performed over 60000 examples with 250 examples per iteration, noise\\nmultiplier 1.3 for 5 epochs with microbatching, and no bound on number of\\nexamples per user.\\n\\nThis privacy guarantee protects the release of all model checkpoints in addition\\nto the final model.\\n\\nExample-level DP with add-or-remove-one adjacency at delta = 1e-05 computed with\\nRDP accounting:\\n    Epsilon with each example occurring once per epoch:        21.254\\n    Epsilon assuming Poisson sampling (*):                      3.782\\n\\nNo user-level privacy guarantee is possible without a bound on the number of\\nexamples per user.\\n\\n(*) Poisson sampling is not usually done in training pipelines, but assuming\\nthat the data was randomly shuffled, it is believed the actual epsilon should be\\ncloser to this value than the conservative assumption of an arbitrary data\\norder.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The epsilon for the differential training comes out to be a total of 3.782**\n",
        "\n",
        "\n",
        "60000 examples with 250 examples per iteration: This indicates that the training dataset consists of 60,000 examples, and during each training iteration, 250 examples are used.\n",
        "\n",
        "Noise multiplier 1.3 for 5 epochs with microbatching: The noise multiplier represents the amount of noise added to the gradient updates to ensure privacy. This value of 1.3 indicates the level of privacy protection. The training is performed for 5 epochs, and microbatching is a technique used to aggregate gradients in smaller batches to improve privacy.\n",
        "\n",
        "Privacy guarantee: The privacy guarantee here protects both the release of model checkpoints during training and the final trained model.\n",
        "\n",
        "Example-level DP with add-or-remove-one adjacency at delta = 1e-05: This appears to be a privacy analysis result using Renyi Differential Privacy (RDP) accounting. It provides an estimate of the privacy level achieved during training.\n",
        "\n",
        "Epsilon with each example occurring once per epoch: This value (21.254) represents the privacy budget (epsilon) when considering each example in the dataset occurring once in each training epoch. It quantifies the overall privacy protection level during training.\n",
        "\n",
        "Epsilon assuming Poisson sampling: This value (3.782) is a more optimistic estimate of the privacy level, assuming that the data was randomly shuffled (Poisson sampling). It suggests that if the data order is not arbitrary and follows some randomness, the actual privacy level could be closer to this value."
      ],
      "metadata": {
        "id": "fKw9_hb5rQWq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6egQPjDBpQYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXw_A3g_0tRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}